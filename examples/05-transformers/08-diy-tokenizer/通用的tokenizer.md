æ²’éŒ¯ï¼è¦å¯¦ä½œä¸€å€‹**é€šç”¨çš„ tokenizer**ï¼Œå°±éœ€è¦æ”¯æ´ä¸åŒé¡å‹çš„ tokenizerï¼Œå› ç‚ºä¸åŒçš„æ¨¡å‹ä½¿ç”¨ä¸åŒçš„åˆ†è©æ–¹å¼ã€‚ä¾‹å¦‚ï¼š  

- **WordPieceï¼ˆBERT, RoBERTaï¼‰**  
- **Byte Pair Encoding (BPE)ï¼ˆGPT-2, T5, Bloomï¼‰**  
- **Unigram Language Modelï¼ˆSentencePieceï¼ŒALBERT, T5ï¼‰**  
- **Character-level Tokenizerï¼ˆGPT-4 çš„è£œå……æ©Ÿåˆ¶ï¼‰**  

ä½ éœ€è¦**æ ¹æ“šæ¨¡å‹é¡å‹é¸æ“‡å°æ‡‰çš„ tokenizer**ï¼Œç„¶å¾Œå¾ Hugging Face ä¸‹è¼‰é©ç”¨çš„è©å½™è¡¨ï¼ˆ`vocab.json`ã€`merges.txt`ã€`tokenizer.json` ç­‰ï¼‰ã€‚  

---

## **é€šç”¨ Tokenizer è¨­è¨ˆ**
ä½ å¯ä»¥è¨­è¨ˆä¸€å€‹**åŸºç¤ `Tokenizer` é¡åˆ¥**ï¼Œç„¶å¾Œè®“ä¸åŒçš„ tokenizer ç¹¼æ‰¿å®ƒï¼Œé€™æ¨£å¯ä»¥åšåˆ°é€šç”¨åŒ–ã€‚

### **æ¶æ§‹è¨­è¨ˆ**
```python
class BaseTokenizer:
    """é€šç”¨ Tokenizer åŸºé¡"""

    def __init__(self, model_name: str):
        self.model_name = model_name
        self.token_to_id = {}
        self.id_to_token = {}

    def load_vocab(self):
        """é€šç”¨è©å½™è¡¨è¼‰å…¥æ–¹æ³•ï¼Œéœ€è¦å„ tokenizer è‡ªè¡Œå¯¦ä½œ"""
        raise NotImplementedError

    def encode(self, text: str):
        """å°‡æ–‡å­—è½‰æ›ç‚º token ID"""
        raise NotImplementedError

    def decode(self, token_ids: list):
        """å°‡ token ID é‚„åŸç‚ºæ–‡å­—"""
        raise NotImplementedError
```
---

## **å…·é«”å¯¦ä½œä¸åŒçš„ tokenizer**

### **1. WordPiece (BERT, RoBERTa)**
```python
class WordPieceTokenizer(BaseTokenizer):
    """å¯¦ä½œ WordPiece åˆ†è©å™¨"""

    def __init__(self, model_name: str = "bert-base-uncased"):
        super().__init__(model_name)
        self.vocab_url = f"https://huggingface.co/{model_name}/resolve/main/vocab.txt"
        self.load_vocab()

    def load_vocab(self):
        """å¾ Hugging Face ä¸‹è¼‰ vocab.txt"""
        import requests
        response = requests.get(self.vocab_url)
        if response.status_code != 200:
            raise ValueError(f"Failed to download vocab from {self.vocab_url}")
        self.token_to_id = {token.strip(): i for i, token in enumerate(response.text.split("\n"))}
        self.id_to_token = {v: k for k, v in self.token_to_id.items()}

    def encode(self, text: str):
        """ä½¿ç”¨ WordPiece åˆ†è©"""
        words = text.lower().split()
        tokens = []
        for word in words:
            if word in self.token_to_id:
                tokens.append(word)
            else:
                sub_tokens = []
                for i in range(len(word)):
                    sub_word = "##" + word[i:] if i > 0 else word[i:]
                    if sub_word in self.token_to_id:
                        sub_tokens.append(sub_word)
                tokens.extend(sub_tokens if sub_tokens else ["[UNK]"])
        return [self.token_to_id[token] for token in tokens]

    def decode(self, token_ids):
        """å°‡ token ID è½‰æ›å›æ–‡å­—"""
        tokens = [self.id_to_token[id] for id in token_ids]
        return " ".join(tokens).replace(" ##", "")
```

---

### **2. BPE (GPT-2, T5)**
```python
class BPETokenizer(BaseTokenizer):
    """å¯¦ä½œ Byte Pair Encoding (BPE) åˆ†è©å™¨"""

    def __init__(self, model_name: str = "gpt2"):
        super().__init__(model_name)
        self.vocab_url = f"https://huggingface.co/{model_name}/resolve/main/vocab.json"
        self.merges_url = f"https://huggingface.co/{model_name}/resolve/main/merges.txt"
        self.bpe_ranks = {}
        self.load_vocab()

    def load_vocab(self):
        """ä¸‹è¼‰ vocab.json å’Œ merges.txt"""
        import requests
        response = requests.get(self.vocab_url)
        if response.status_code != 200:
            raise ValueError(f"Failed to download vocab from {self.vocab_url}")
        self.token_to_id = json.loads(response.text)
        self.id_to_token = {v: k for k, v in self.token_to_id.items()}

        response = requests.get(self.merges_url)
        if response.status_code != 200:
            raise ValueError(f"Failed to download merges from {self.merges_url}")
        merges = response.text.strip().split("\n")[1:]
        self.bpe_ranks = {tuple(pair.split()): i for i, pair in enumerate(merges)}

    def _bpe(self, word):
        """æ‡‰ç”¨ BPE åˆ†è©"""
        tokens = list(word)
        while len(tokens) > 1:
            pairs = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]
            pair_ranks = {pair: self.bpe_ranks[pair] for pair in pairs if pair in self.bpe_ranks}
            if not pair_ranks:
                break
            best_pair = min(pair_ranks, key=pair_ranks.get)
            new_tokens = []
            i = 0
            while i < len(tokens):
                if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == best_pair:
                    new_tokens.append("".join(best_pair))
                    i += 2
                else:
                    new_tokens.append(tokens[i])
                    i += 1
            tokens = new_tokens
        return tokens

    def encode(self, text):
        words = text.lower().split()
        tokens = [subword for word in words for subword in self._bpe(word)]
        return [self.token_to_id.get(token, self.token_to_id.get("<|endoftext|>")) for token in tokens]

    def decode(self, token_ids):
        tokens = [self.id_to_token[id] for id in token_ids]
        return "".join(tokens).replace("Ä ", " ")
```

---

### **3. Unigram SentencePiece (T5, ALBERT)**
```python
class SentencePieceTokenizer(BaseTokenizer):
    """å¯¦ä½œ SentencePiece Tokenizer"""

    def __init__(self, model_name: str = "t5-small"):
        super().__init__(model_name)
        self.tokenizer_url = f"https://huggingface.co/{model_name}/resolve/main/tokenizer.json"
        self.load_vocab()

    def load_vocab(self):
        """ä¸‹è¼‰ tokenizer.json"""
        import requests
        response = requests.get(self.tokenizer_url)
        if response.status_code != 200:
            raise ValueError(f"Failed to download tokenizer.json from {self.tokenizer_url}")
        tokenizer_data = json.loads(response.text)
        self.token_to_id = tokenizer_data["model"]["vocab"]
        self.id_to_token = {v: k for k, v in self.token_to_id.items()}

    def encode(self, text):
        """ç›´æ¥æŸ¥æ‰¾è©å½™è¡¨"""
        tokens = text.lower().split()
        return [self.token_to_id.get(token, self.token_to_id.get("<unk>")) for token in tokens]

    def decode(self, token_ids):
        return " ".join([self.id_to_token[id] for id in token_ids])
```

---

## **æœ€çµ‚é€šç”¨ Tokenizer**
```python
def get_tokenizer(model_name: str):
    if "bert" in model_name or "roberta" in model_name:
        return WordPieceTokenizer(model_name)
    elif "gpt2" in model_name or "t5" in model_name:
        return BPETokenizer(model_name)
    elif "t5" in model_name or "albert" in model_name:
        return SentencePieceTokenizer(model_name)
    else:
        raise ValueError("Unsupported model type")

tokenizer = get_tokenizer("gpt2")
print(tokenizer.encode("Hello world! GPT-2 is amazing."))
```

é€™æ¨£ä½ çš„ tokenizer å°±å¯ä»¥é©ç”¨æ–¼ Hugging Face ä¸‹è¼‰çš„ä»»ä½•æ¨¡å‹ï¼ ğŸš€