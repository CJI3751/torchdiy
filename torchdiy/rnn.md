* https://chatgpt.com/c/67c5055b-8884-8012-98b4-9840fb53d7b8

這段程式碼的核心數學原理來自於 **循環神經網絡（Recurrent Neural Networks, RNN）**，並包含了 **基本 RNN（Simple RNN）** 和 **門控循環單元（GRU, Gated Recurrent Unit）** 的數學計算。以下是各部分的數學推導：

---

## **1. 簡單 RNN 的數學原理**
簡單 RNN 的數學公式如下：

\[
h_t = \sigma(W_{ih} x_t + b_{ih} + W_{hh} h_{t-1} + b_{hh})
\]

其中：
- \( h_t \) 是當前時間步的隱藏狀態（hidden state）
- \( x_t \) 是當前時間步的輸入
- \( W_{ih} \) 是輸入到隱藏層的權重
- \( W_{hh} \) 是隱藏狀態到隱藏狀態的遞歸權重
- \( b_{ih} \), \( b_{hh} \) 是對應的偏置
- \( \sigma \) 是激活函數，程式碼中支援 `tanh` 或 `relu`

**核心計算：**
- 每個時間步 \( t \)，隱藏狀態 \( h_t \) 依賴於前一個時間步的隱藏狀態 \( h_{t-1} \) 和當前的輸入 \( x_t \)。
- 權重 \( W_{ih} \) 和 \( W_{hh} \) 是訓練時學習的參數。
- **計算流程：**
  1. 計算輸入 \( x_t \) 經過輸入權重 \( W_{ih} \) 和偏置 \( b_{ih} \)
  2. 計算前一時間步的隱藏狀態 \( h_{t-1} \) 經過隱藏層權重 \( W_{hh} \) 和偏置 \( b_{hh} \)
  3. 將兩者相加，並通過 `tanh` 或 `relu` 激活函數

這個方法存在 **梯度消失（vanishing gradient）** 的問題，當序列變長時，遠端時間步的信息很難影響當前時間步。

---

## **2. GRU（門控循環單元）數學原理**
GRU 是一種改進的 RNN，它通過 **重置門（reset gate）和更新門（update gate）** 控制信息流動。數學公式如下：

### **(1) 更新門 \( z_t \)**
\[
z_t = \sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{t-1} + b_{hz})
\]
- \( z_t \) 決定了前一時間步的隱藏狀態 \( h_{t-1} \) 有多少被保留到當前狀態 \( h_t \)。
- \( \sigma \) 是 sigmoid 函數，將輸出壓縮到 \( (0,1) \) 之間。

### **(2) 重置門 \( r_t \)**
\[
r_t = \sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{t-1} + b_{hr})
\]
- \( r_t \) 控制前一時間步的狀態 \( h_{t-1} \) 有多少被丟棄。

### **(3) 候選隱藏狀態 \( \tilde{h_t} \)**
\[
\tilde{h_t} = \tanh(W_{in} x_t + b_{in} + r_t \odot (W_{hn} h_{t-1} + b_{hn}))
\]
- \( \tilde{h_t} \) 是一個候選的隱藏狀態，它受到重置門 \( r_t \) 控制，只有當 \( r_t \) 接近 1 時，才會完全保留 \( h_{t-1} \) 的信息。

### **(4) 最終隱藏狀態 \( h_t \)**
\[
h_t = (1 - z_t) \odot \tilde{h_t} + z_t \odot h_{t-1}
\]
- 當 \( z_t \) 趨近 1 時，\( h_t \) 更傾向於保留 \( h_{t-1} \)，讓過去的信息不會過快遺忘。
- 當 \( z_t \) 趨近 0 時，\( h_t \) 更傾向於接受新的信息 \( \tilde{h_t} \)。

### **GRU 相比簡單 RNN 的優勢**
- **可控的長期記憶**：透過 \( z_t \) 門控機制，GRU 可以在序列中保留更長時間的資訊，解決梯度消失問題。
- **計算量較少**：GRU 只使用兩個門（相比 LSTM 的三個門），因此計算開銷較小。

---

## **3. 多層 RNN**
程式中的 `MultiLayerRNN` 支援：
1. 多層堆疊：讓不同層次的特徵學習更高級的抽象表示。
2. 雙向（Bidirectional）：可以同時考慮時間序列的前後關係。

假設我們有 \( L \) 層，每層的隱藏狀態為 \( h_t^l \)，則：

\[
h_t^l = \sigma(W_{ih}^l x_t + W_{hh}^l h_{t-1}^l)
\]

當多層 RNN 開啟雙向（bidirectional）時，每個時間步的最終輸出為：

\[
h_t = [h_t^{\text{forward}}, h_t^{\text{backward}}]
\]

這樣可以同時考慮過去與未來的資訊，在自然語言處理等任務中特別有用。

---

## **總結**
這段程式碼實作了：
1. **簡單 RNN**：
   - \( h_t = \sigma(W_{ih} x_t + W_{hh} h_{t-1} + b) \)
   - 適合短序列，但有梯度消失問題。
   
2. **GRU**：
   - 引入 **重置門** 和 **更新門** 來控制信息流動。
   - 更擅長學習長期依賴。

3. **多層 RNN**：
   - 堆疊多層，提高表達能力。
   - 可選擇雙向（Bidirectional）學習未來信息。

這些方法在自然語言處理（如機器翻譯）、時間序列預測（如股價預測）等領域非常重要！