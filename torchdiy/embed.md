* https://chatgpt.com/c/67c5055b-8884-8012-98b4-9840fb53d7b8

é€™æ®µç¨‹å¼ç¢¼çš„æ ¸å¿ƒæ˜¯ **Embedding (åµŒå…¥å±¤)**ï¼Œå®ƒçš„ä¸»è¦æ•¸å­¸åŸç†ä¾†è‡ªæ–¼ **æŸ¥è¡¨é‹ç®— (Lookup Table) å’Œ åµŒå…¥å‘é‡ (Word Embeddings)**ï¼Œé€™æ˜¯æ·±åº¦å­¸ç¿’ä¸­è™•ç†é›¢æ•£è³‡æ–™ (å¦‚æ–‡å­—ã€åˆ†é¡è®Šæ•¸) çš„é—œéµæŠ€è¡“ã€‚

---

## **1. Embedding (åµŒå…¥å±¤) çš„æ•¸å­¸æ¦‚å¿µ**
### **(1) æŸ¥è¡¨ (Lookup Table)**
å‡è¨­æˆ‘å€‘æœ‰ä¸€å€‹è©å½™è¡¨ (Vocabulary)ï¼š
\[
V = \{ \text{"apple"}, \text{"banana"}, \text{"cherry"}, \text{"date"} \}
\]
ä¸¦å°‡æ¯å€‹å–®è©å°æ‡‰åˆ°ä¸€å€‹ç´¢å¼•ï¼š
\[
\text{"apple"} \to 0, \quad \text{"banana"} \to 1, \quad \text{"cherry"} \to 2, \quad \text{"date"} \to 3
\]
é‚£éº¼ï¼Œæˆ‘å€‘çš„åµŒå…¥çŸ©é™£ (Weight Matrix) \( W \) å°±æ˜¯ä¸€å€‹å½¢ç‹€ç‚º \( (|V|, d) \) çš„å¼µé‡ï¼Œå…¶ä¸­ \( |V| \) æ˜¯è©å½™æ•¸é‡ï¼Œ\( d \) æ˜¯åµŒå…¥ç¶­åº¦ã€‚ä¾‹å¦‚ï¼š
\[
W = \begin{bmatrix}
\mathbf{w}_0 \\ 
\mathbf{w}_1 \\ 
\mathbf{w}_2 \\ 
\mathbf{w}_3
\end{bmatrix}
=
\begin{bmatrix}
0.1 & 0.2 & 0.3 \\
0.4 & 0.5 & 0.6 \\
0.7 & 0.8 & 0.9 \\
1.0 & 1.1 & 1.2
\end{bmatrix}
\]
å¦‚æœæˆ‘å€‘çš„è¼¸å…¥æ˜¯ç´¢å¼• `input = [1, 3]`ï¼Œå‰‡åµŒå…¥å±¤æœƒå¾ \( W \) ä¸­é¸å–ç´¢å¼• 1 å’Œ 3 çš„å‘é‡ï¼š
\[
\text{output} = 
\begin{bmatrix}
\mathbf{w}_1 \\
\mathbf{w}_3
\end{bmatrix}
=
\begin{bmatrix}
0.4 & 0.5 & 0.6 \\
1.0 & 1.1 & 1.2
\end{bmatrix}
\]
é€™å°æ‡‰åˆ°ç¨‹å¼ç¢¼ï¼š
```python
output = weight[input]
```
é€™å¯¦éš›ä¸Šæ˜¯ PyTorch æä¾›çš„é«˜æ•ˆæŸ¥è¡¨æ“ä½œã€‚

---

### **(2) Padding (å¡«å……)**
ç•¶ä½¿ç”¨ `padding_idx` åƒæ•¸æ™‚ï¼Œå°æ‡‰ç´¢å¼•çš„åµŒå…¥å‘é‡å°‡è¢«è¨­ç‚ºé›¶ã€‚ä¾‹å¦‚ï¼Œè‹¥ `padding_idx=0`ï¼Œå‰‡ï¼š
\[
\mathbf{w}_0 = \mathbf{0}
\]
å°æ‡‰çš„ç¨‹å¼ç¢¼ï¼š
```python
if padding_idx is not None:
    mask = (input == padding_idx).unsqueeze(-1)
    output = output.masked_fill(mask, 0.0)
```
é€™ç¢ºä¿äº†å¡«å……ç´¢å¼•ä¸æœƒå½±éŸ¿æ¨¡å‹çš„å­¸ç¿’ã€‚

---

### **(3) åµŒå…¥å‘é‡æ­£è¦åŒ– (Normalization)**
å¦‚æœè¨­å®š `max_norm`ï¼Œå‰‡åµŒå…¥å‘é‡çš„ç¯„æ•¸ \( ||\mathbf{w}|| \) ä¸è¶…é `max_norm`ï¼š

\[
\mathbf{w} \leftarrow \mathbf{w} \cdot \min\left( \frac{\text{max_norm}}{||\mathbf{w}||}, 1 \right)
\]

ç¨‹å¼ç¢¼ï¼š
```python
if max_norm is not None:
    norms = output.norm(p=norm_type, dim=-1, keepdim=True)
    output = output * torch.clamp(max_norm / norms, max=1.0)
```
é€™ç¢ºä¿æ‰€æœ‰åµŒå…¥å‘é‡çš„é•·åº¦ä¸è¶…é `max_norm`ï¼Œæœ‰åŠ©æ–¼ç©©å®šè¨“ç·´ã€‚

---

## **2. PyTorch `nn.Embedding` çš„ç­‰æ•ˆå¯¦ä½œ**
åœ¨ `Embedding` é¡åˆ¥ä¸­ï¼Œ`self.weight` æ˜¯ä¸€å€‹å½¢ç‹€ç‚º `(num_embeddings, embedding_dim)` çš„ learnable parameterï¼š
```python
self.weight = nn.Parameter(torch.Tensor(num_embeddings, embedding_dim))
```
åˆå§‹æ™‚ï¼ŒåµŒå…¥å‘é‡æœƒéš¨æ©Ÿåˆå§‹åŒ–ï¼š
```python
nn.init.normal_(self.weight)
self.weight.data.mul_(1.0 / math.sqrt(self.embedding_dim))
```
é€™éµå¾ªäº† **word2vec** çš„åˆå§‹åŒ–æ–¹å¼ï¼Œä½¿åµŒå…¥å€¼çš„æ¨™æº–å·®ç‚º \( \frac{1}{\sqrt{\text{embedding_dim}}} \)ã€‚

ç•¶æ¨¡å‹è¨“ç·´æ™‚ï¼Œé€™äº›åµŒå…¥å‘é‡æœƒæ ¹æ“šæå¤±å‡½æ•¸å’Œæ¢¯åº¦ä¸‹é™æ³•é€²è¡Œèª¿æ•´ï¼Œè®“æ¨¡å‹å­¸ç¿’æœ€é©åˆçš„è©å‘é‡ã€‚

---

## **3. ç›¸é—œæ‡‰ç”¨**
1. **è‡ªç„¶èªè¨€è™•ç† (NLP)**ï¼š
   - å°‡è©å½™è½‰æ›ç‚ºå‘é‡ï¼Œå¦‚ Word2Vecã€GloVeã€FastTextã€BERT ç­‰ã€‚
   - ä¾‹å¦‚ï¼Œåœ¨ LSTM æˆ– Transformer çš„è©åµŒå…¥å±¤ã€‚
   
2. **åˆ†é¡å•é¡Œ**ï¼š
   - ç”¨æ–¼è™•ç† **é¡åˆ¥è®Šæ•¸** (å¦‚é›»å½±é¡å‹ã€ç”¨æˆ¶ ID)ã€‚
   - ä¾‹å¦‚ï¼Œåœ¨æ¨è–¦ç³»çµ±ä¸­ï¼ŒæŠŠç”¨æˆ¶ ID è½‰æ›æˆåµŒå…¥å‘é‡ä¾†è¨ˆç®—ç›¸ä¼¼æ€§ã€‚

---

## **4. ç¸½çµ**
é€™æ®µç¨‹å¼ç¢¼å¯¦ä½œäº†ä¸€å€‹èˆ‡ `torch.nn.Embedding` ç­‰æ•ˆçš„ **æ‰‹å¯«åµŒå…¥å±¤**ï¼Œå…¶æ•¸å­¸åŸç†åŒ…å«ï¼š
- **æŸ¥è¡¨é‹ç®—**ï¼šå¾è©å½™ç´¢å¼•æå–å°æ‡‰çš„åµŒå…¥å‘é‡ã€‚
- **Padding**ï¼šç‚ºç‰¹æ®Šç´¢å¼•è¨­ç½®é›¶å‘é‡ï¼Œé¿å…å½±éŸ¿è¨ˆç®—ã€‚
- **ç¯„æ•¸æ­£è¦åŒ–**ï¼šç¢ºä¿åµŒå…¥å‘é‡é•·åº¦ä¸è¶…é `max_norm`ã€‚
- **åƒæ•¸å­¸ç¿’**ï¼šåœ¨æ·±åº¦å­¸ç¿’è¨“ç·´éç¨‹ä¸­ï¼Œè‡ªå‹•èª¿æ•´åµŒå…¥å‘é‡çš„å€¼ã€‚

é€™æ˜¯ NLP å’Œæ¨è–¦ç³»çµ±ä¸­ **é›¢æ•£è³‡æ–™å‘é‡åŒ–** çš„é—œéµæŠ€è¡“ï¼ğŸš€